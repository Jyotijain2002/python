import numpy as np
import matplotlib.pyplot as plt
from lab_utils_multi import zscore_normalize_features,run_gradient_descent_feng

'''
in linear regression the features are linear so we try to fiut a straight line into the data
but 
what if the features or data are non linear or a combination of features
for example housing price do not tend to be linear with area(x*x)
for that we need polynomial regression with helps us to fit the curves into the data with non linear features 
'''
# y=1+x**2

x=np.arange(0,20,1)
y=1+x**2
X=x.reshape(-1,1)

model_w,model_b=run_gradient_descent_feng(X,y,1000,1e-2)
plt.scatter(x,y,marker='x',c='r',label="actual_values")
plt.title("no feature engineering")
plt.plot(x,X@model_w + model_b, label="Predicted Value");  plt.xlabel("X"); plt.ylabel("y")
plt.legend()
plt.show()

#as expected not a great fit, we need to, we needed something like wx**2+b or a polynomial feature


# create target data
x = np.arange(0, 20, 1)
y = 1 + x**2

# Engineer features 
X = x**2      #<-- added engineered feature
X = X.reshape(-1, 1)  #X should be a 2-D Matrix
model_w,model_b = run_gradient_descent_feng(X, y, iterations=10000, alpha = 1e-5)

plt.scatter(x, y, marker='x', c='r', label="Actual Value"); plt.title("Added x**2 feature")
plt.plot(x, np.dot(X,model_w) + model_b, label="Predicted Value"); plt.xlabel("x"); plt.ylabel("y"); plt.legend(); plt.show()


'''
X=np.c_[a,b,c]: it concatenate this a,b,c list column wise
eg: a=[1,2],b=[3,4],c=[5,6]
X=[[1,3,5],
   [2,4,6]]
'''

# selecting features:
'''
above we know that x**2 feature was required, but it may not always so obvious which features are required, so would try to add variety of potential features to try and find the most useful.
'''
x=np.arange(0,20,1)
y=x**2
X=np.c_[x,x**2,x**3] # <--added engineered features

model_w,model_b=run_gradient_descent_feng(X,y,10000,1e-7)

plt.scatter(x,y,marker='x',c='r',label="actual vales")
plt.title('x,x**2,x**3 features')
plt.plot(x,X@model_w+model_b,label="predicted values")
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()

'''
Note the value of  洧냟
 , [0.08 0.54 0.03] and b is 0.0106.This implies the model after fitting/training is:
0.08洧논+0.54洧논2+0.03洧논3+0.0106
 
Gradient descent has emphasized the data that is the best fit to the  洧논2
  data by increasing the  洧녻1
  term relative to the others. If you were to run for a very long time, it would continue to reduce the impact of the other terms.

Gradient descent is picking the 'correct' features for us by emphasizing its associated parameter

Let's review this idea:

less weight value implies less important/correct feature, and in extreme, when the weight becomes zero or very close to zero, the associated feature is not useful in fitting the model to the data.
above, after fitting, the weight associated with the  洧논2
  feature is much larger than the weights for  洧논
  or  洧논3
  as it is the most useful in fitting the data.
'''

# an alternate view
x=np.arange(0,20,1)
y=x**2

#enginering features
X=np.c_[x,x**2,x**3]
X_features=['X','X^2','X^3']

fig,ax=plt.subplots(1,3,figsize=(12,3),sharey=True)
for i in range(len(ax)):
    ax[i].scatter(X[:,i],y)
    ax[i].set_xlabel(X_features[i])
ax[0].set_ylabel('y')
plt.show()

'''
Above, polynomial features were chosen based on how well they matched the target data. Another way to think about this is to note that we are still using linear regression once we have created new features. Given that, the best features will be linear relative to the target. This is best understood with an example.
Above, it is clear that the  洧논2
  feature mapped against the target value  洧녽
  is linear. Linear regression can then easily generate a model using that feature.
'''

#scaling features
'''
x,x^2,x^3 will have different scales, so we need to do feature scaling to find the gradient descent fast, so lets apply z-score normalization
'''

x=np.arange(0,20,1)
X=np.c_[x,x**2,x**3]
print(f'peek to peek range by column in raw   X:{np.ptp(X,axis=0)}')

# add normalization
X=zscore_normalize_features(X)
print(f'peak to peak range by column in normalized X: {np.ptp(X,axis=0)}')

# after normalization we can try again with more aggressive value of alpha

model_w,model_b= run_gradient_descent_feng(X,y,100000,1e-1)

plt.scatter(x,y,marker='x',c='r',label='actual value')
plt.title("Normalized x x**2, x**3 feature")

plt.plot(x,X@model_w+model_b, label='predicted values')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
'''
Feature scaling allows this to converge much faster.
Note again the values of  洧냟
 . The  洧녻1
  term, which is the  洧논2
  term is the most emphasized. Gradient descent has all but eliminated the  洧논3
  term.
'''

#complex functions
x=np.arange(0,20,1)
y=np.cos(x/2)
X=np.c_[x, x**2, x**3,x**4, x**5, x**6, x**7, x**8, x**9, x**10, x**11, x**12, x**13]
X=zscore_normalize_features(X)
model_w,model_b=run_gradient_descent_feng(X,y,1000000,1e-1)

plt.scatter(x, y, marker='x', c='r', label="Actual Value"); plt.title("Normalized x x**2, x**3 feature")
plt.plot(x,X@model_w + model_b, label="Predicted Value"); plt.xlabel("x"); plt.ylabel("y"); plt.legend(); plt.show()